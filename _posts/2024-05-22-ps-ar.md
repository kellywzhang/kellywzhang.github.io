---
layout: distill
title: Posterior Sampling via Autoregressive Generation
description: Bridging Generative Sequence Modeling and Online Decision-Making
tags: bandit-algorithms, Thompson-sampling, generative-models
giscus_comments: true
date: 2024-05-29
featured: false

authors:
  - name: Kelly W. Zhang*
    url: "https://kellywzhang.github.io/"
    affiliations:
      name: Columbia University
  - name: Tiffany Cai*
    url: "https://tc2718.github.io/"
    affiliations:
      name: Columbia University
  - name: Hongseok Namkoong
    url: "https://hsnamkoong.github.io/"
    affiliations:
      name: Columbia University
  - name: Daniel Russo
    url: "https://djrusso.github.io/"
    affiliations:
      name: Columbia University

bibliography: 2024-05-24-psar.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Introduction
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Equation

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

### Introdution

Real-world decision-making often involves grappling with uncertainty This challenge is particularly acute in areas like recommendation systems, where new items are continuously introduced or BLAH BLAH. Traditional bandit algorithms struggle to handle the complex, unstructured data (e.g., text and images) that characterizes modern applications. This work introduces a novel framework for learning bandit algorithms using autoregressive sequence models, offering a scalable and principled approach to Thompson sampling.

### Our Approach

#### We propose a two-phase solution:

__Pretraining:__ We leverage historical data to train an autoregressive sequence model that predicts user feedback (e.g., clicks) on articles. The model learns a prior distribution over the engagement of articles based on their features (e.g., headlines).

__Online Decision-Making:__ At decision time, we use the pretrained model to autoregressively sample imaginary sequences of user feedback for each article. We then choose the article with the highest average imputed reward, representing a sample from the posterior distribution of the article's true engagement.

### Key Insights:

- This approach is a principled implementation of Thompson sampling, a powerful algorithm for balancing exploration and exploitation.
- By sampling from the posterior distribution of article rewards, we directly address the uncertainty inherent in the decision-making process.
- Our framework leverages advances in autoregressive generative models (like transformers), allowing for scalability and integration with rich feature representations (e.g., article headlines).

### Theoretical Foundations
We prove a novel regret bound that links the online performance of our algorithm to the prediction loss of the pretrained sequence model. This provides a strong theoretical foundation for our approach.

### Empirical Evaluation:

We demonstrate our method on both synthetic and real-world news recommendation tasks. Our results show:
__Improved performance:__ Our approach outperforms traditional bandit algorithms, especially when using rich features like article headlines.

__Uncertainty quantification:__ We can effectively estimate the uncertainty associated with article engagement, allowing for more informed decision-making.

### Conclusion:
By framing bandit problems as sequence modeling tasks, we have introduced a novel and scalable approach to Thompson sampling that is well-suited for modern, complex applications. Our framework leverages the power of autoregressive generative models, providing a strong theoretical foundation and promising empirical results.

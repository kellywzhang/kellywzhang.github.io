---
layout: distill
title: Posterior Sampling via Autoregressive Generation
description: Bridging Generative Sequence Modeling and Online Decision-Making
tags: bandit-algorithms, Thompson-sampling, generative-models
giscus_comments: false
date: 2024-05-29
featured: false

authors:
  - name: Kelly W. Zhang*
    url: "https://kellywzhang.github.io/"
    affiliations:
      name: Columbia University
  - name: Tiffany Cai*
    url: "https://tc2718.github.io/"
    affiliations:
      name: Columbia University
  - name: Hongseok Namkoong
    url: "https://hsnamkoong.github.io/"
    affiliations:
      name: Columbia University
  - name: Daniel Russo
    url: "https://djrusso.github.io/"
    affiliations:
      name: Columbia University

bibliography: 2024-05-24-psar.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:  
    - name: Problem Setting
    - name: Our Approach
    - name: Key Insights
    - name: Empirical Evaluation
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

Real-world decision-making often involves grappling with uncertainty. This challenge is particularly acute in areas like recommendation systems, where new items are continuously introduced, and personalized user experiences where new individual consistently enter the system. Traditional bandit algorithms struggle to handle the complex, unstructured data (e.g., text and images) that are common in modern applications. This work introduces a novel framework for learning bandit algorithms using autoregressive sequence models, offering a scalable and principled approach to Thompson sampling.

### Problem Setting
Formally, we define meta-learning bandit problem where the bandit repeatedly encounters new tasks that require exploring to gather useful information. We illustrate our approach using a news article recommendation setting. 

Each day, new articles (actions) are released, and the system needs to recommend these to users while balancing exploration (trying new articles) and exploitation (showing the articles predicted to be the most popular based on the article headline text). Although article headlines provide a useful early indicator of article performance, models that solely rely on such features will eventually be outperformed by simple alternatives that learn from actual user feedback. The algorithm also has access to a large historical recommendation dataset on previously released articles from past days.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/psar/data-interactions4.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
Our meta-bandit setting has two stages: (1) learning from a massive historical dataset and (2) online decision-making on a new task (recommendation problem with new items).
</div>


### Our Approach

The impetus of our approach is that **unobserved outcome data** is the source of the decision-maker's uncertainty: feedback on an article has only been gathered from a subset of users, and there is residual uncertainty in how others users would respond.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/psar/multiple_imputation2.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
We view uncertainty about unobserved outcomes as the source of uncertainty, avoiding explicit reference to latent parameters or variables. Calibrated generation (imputation) of missing outcomes enables uncertainty quantification and decision-making.
</div>

Specifically, we propose a two-phase approach:

__(1) Pretraining:__ Use the historical recommendation data to train an autoregressive sequence model $p_\theta$ that predicts successive user responses (e.g., clicks) to an article. This sequence model also takes in the article headline as input (via a pre-trained LLM that is fine-tuned). 

[comment]: <> ($$\ell(p_\theta ; \mathcal{D}^{\mathrm{hist}} = - \sum_{a \in \mathcal{A}^{\mathrm{hist}}}  \sum_{t=1}^{n} \log p_{\theta}(Y_t^{(a)} \mid Z^{(a)}, Y_{1:t-1}^{(a)}).$$)

__(2) Online Decision-Making:__ At decision time, we use the pretrained model to autoregressively sample (impute) imaginary sequences of user feedback for each article. We then choose the article with the highest average imputed reward, representing a sample from the posterior distribution of the article's true average reward.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/psar/posterior_sampling_implementation3.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
Posterior Sampling via Autoregressive Generation (PS-AR). PS-AR uses autoregressive generation (imputation) of unobserved potential outcomes to (implicitly) reason about uncertainty and drive exploration of arms that could plausibly be optimal. After imputing all missing outcomes, it forms an imputed mean reward per action and selects the action with the highest imputed mean.
</div>


### Key Insights:

- __Implementation of Thompson Sampling:__ We show that this Posterior Sampling via Autoregressive Generation approach is a principled implementation of Thompson sampling (with a learned prior), a powerful algorithm for balancing exploration and exploitation.
- __Regret Bound:__ We prove a novel regret bound that links the online performance of our algorithm to the prediction loss of the pretrained sequence model. This provides a strong theoretical foundation for our approach.
- __Pretraining as Empirical Bayes:__: We show that for particular choices for sequence model classes minimizing our pretraining loss is equivalent to maximizing the marginal likelihood, the criterion that is used in Empirical Bayes, a popular approach to fitting a prior distributions to observed data. We find in experiments that training on our sequence loss can recover the true Bayesian prior. Our pretraining procedure can be viewed as learning an approximate Bayesian model by gradient descent. 
- __Advantages of Autorgressive Viewpoint__: Our framework leverages advances in autoregressive generative models (like transformers), allowing for scalability and integration with rich feature representations (e.g., article headlines).


### Empirical Evaluation:

We demonstrate our method on both synthetic and real-world news recommendation tasks. Our results show:
- __Improved decision-making:__ Our approach outperforms traditional bandit algorithms by taking advantage of rich action features like article headlines.
- __Uncertainty quantification:__ Our method allows us to effectively model uncertainty in the mean reward for each article. We use our pretrained sequence model to construct credible intervals for the mean rewards and find they have extremely well calibrated coverage. 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/psar/mind_eval.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
Left: In terms of cumulative regret, the PS-AR models that use sequence models $p_\theta$ that incorporate text features outperform all other algorithms.
Right: We form credible intervals for the mean reward for each article/action by autoregressively sampling (imputing) many hypothetical sequences of future rewards given the article headline.
</div>
